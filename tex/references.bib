% ei vertais
@book{TheArtOfApplication,
abstract = {Because performance is paramount today, this thoroughly updated guide shows you how to test mission-critical applications for scalability and performance before you deploy themwhether its to the cloud or a mobile device. Youll learn the complete testing process lifecycle step-by-step, along with best practices to plan, coordinate, and conduct performance tests on your applications.Set realistic performance testing goalsImplement an effective application performance testing strategyInterpret performance test resultsCope with different application technologies and architecturesUnderstand the importance of End User Monitoring (EUM)Use automated performance testing toolsTest traditional local applications, web applications, and web servicesRecognize and resolves issues often overlooked in performance testsWritten by a consultant with over 15 years experience with performance testing, The Art of Application Performance Testing thoroughly explains the pitfalls of an inadequate testing strategy and offers a robust, structured approach for ensuring that your applications perform well and scale effectively when the need arises.},
author = {Molyneaux, Ian},
address = {Sebastopol},
booktitle = {The Art of Application Performance Testing},
isbn = {1491900547},
keywords = {Application software ; Web services},
language = {eng},
publisher = {O'Reilly Media, Incorporated},
title = {The Art of Application Performance Testing: From Strategy to Tools},
year = {2015},
}

% vertais
@article{ComparativeAnalysisOfWeb, 
title={Comparative analysis of web application performance testing tools}, volume={17}, url={https://ph.pollub.pl/index.php/jcsi/article/view/2209}, DOI={10.35784/jcsi.2209}, 
abstractNote={&lt;p&gt;Recent years have brought the rise of importance of quality of developed software. Web applications should be functional, user friendly as also efficient. There are many tools available on the market for testing the performance of web applications. To help you choose the right tool, the article compares three of them: Apache JMeter, LoadNinja and Gatling. They were analyzed in terms of a user-friendly interface, parameterization of the requests and creation of own testing scripts. The research was carried out using a specially prepared application. The summary indicates the most important advantages and disadvantages of the selected tools.&lt;/p&#38;gt;}, 
journal={Journal of Computer Sciences Institute}, 
author={Kołtun, Agata and Pańczyk, Beata}, 
year={2020}, 
month={12}, 
pages={351-357},
doi={10.35784/jcsi.2209}
}

% ei vertais
@inproceedings{PerformanceTestingAnalyzingFifferencesOfResponseTime,
abstract = {This research focuses on the study and evaluation of response time differences given by three tools used for performance testing. The motivation for this research work is to understand the behavior of various performance testing tools towards determining the accuracy of the response time result. It is conducted with the aim of demonstrating and proving that differences of response time do exist between different tools when conducting performance test for the same webpage as well as analyzing the reasons behind that situation. A static HTML webpage is put under load test for 1, 100, 200, 300, 400 and 500 concurrent users performed by the three tools. The findings clearly showed that different performance testing tool gave different response time when conducting load testing on same webpage. The findings are also supported with the justification for these differences, which involve architecture and simulation mechanism of the respective tool. The summary and future work is presented at the end of the research.},
author = {Suffian, Muhammad Dhiauddin Mohamed and Fahrurazi, F. R.},
booktitle = {2012 International Conference on Computer \& Information Science (ICCIS)},
isbn = {1467319376},
keywords = {Benchmark testing ; concurrent users ; HTML ; load test ; Measurement ; open source tool ; performance testing ; Random access memory ; Relays ; response time ; Servers ; Time factors},
language = {eng},
pages = {919-923},
publisher = {IEEE},
title = {Performance testing: Analyzing differences of response time between performance testing tools},
volume = {2},
year = {2012},
doi = {10.1109/ICCISci.2012.6297157}
}

% ehkä vertais?
@article{SoftwareAndPerformanceTestingTools,
abstract = {Software Testing may be a method, that involves, death penalty of a software system program/application and finding all errors or bugs therein program/application in order that the result is going to be a defect-free software system. Quality of any software system will solely be acknowledged through means that of testing (software testing). Through the advancement of technology round the world, there inflated the quantity of verification techniques and strategies to check the software system before it goes to production and astray to promote. Automation Testing has created its impact within the testing method. Now-a-days, most of the software system testing is finished with the automation tools that not solely lessens the quantity of individuals operating around that software system however additionally the errors which will be loose through the eyes of the tester. Automation take look acting contains test cases that make the work simple to capture totally different eventualities and store them. Therefore, software system automation testing method plays a significant role within the software system testing success. This study aims in knowing differing kinds of software system testing, software system testing techniques and tools and to match manual testing versus automation testing.},
author = {Srivastava, Nishi},
issn = {2582-7006},
journal = {Journal of Informatics Electrical and Electronics Engineering (JIEEE)},
language = {eng},
number = {1},
organization = {Department of Computer Science and Engineering, Amity University Uttar Pradesh Lucknow Campus, India},
pages = {1-12},
title = {Software and Performance Testing Tools},
volume = {2},
year = {2021},
doi = {10.54060/JIEEE/002.01.001}
}

% ei vertais
@book{PerformanceTestingGuidanceForWebApplications,
  title={Performance testing guidance for web applications: patterns \& practices},
  author={Meier, J and Farre, Carlos and Bansode, Prashant and Barber, Scott and Rea, Dennis},
  year={2007},
  publisher={Microsoft press}
}

% vertais
@inproceedings{EfficientExperimentSelection,
author = {Westermann, Dennis and Krebs, Rouven and Happe, Jens},
address = {Berlin, Heidelberg},
booktitle = {Computer Performance Engineering},
year="2011",
copyright = {Springer-Verlag Berlin Heidelberg 2011},
isbn = {3642247482},
issn = {0302-9743},
keywords = {Enterprise Application ; Experiment Selection ; Mean Relative Error ; Multivariate Adaptive Regression Spline ; Software Performance},
language = {eng},
pages = {325-339},
publisher = {Springer Berlin Heidelberg},
series = {Lecture Notes in Computer Science},
title = {Efficient Experiment Selection in Automated Software Performance Evaluations},
doi = {10.1007/978-3-642-24749-1_24}
}

% vertais
@article{StudyOfTestScriptFesignMethods,
abstract = {Web Service interface technology is more and more widely applied in information system. And more requirements of Web Service performance testing are demanded. Nevertheless, designing the test script for performance testing is hard to take into practice. In this paper, two kinds of test script design methods for Web Service performance testing are presented by using LoadRunner and SOAP UI tools. That is The Service Call Method and The SOAP Method.},
author = {Xu, Peng},
address = {Bristol},
copyright = {Published under licence by IOP Publishing Ltd},
issn = {1757-8981},
journal = {IOP conference series. Materials Science and Engineering},
keywords = {Design techniques ; Soaps ; Web services},
language = {eng},
number = {1},
pages = {012019},
publisher = {IOP Publishing},
title = {Study of test script design methods for Web Service performance testing},
volume = {231},
year = {2017},
doi={10.1088/1757-899X/231/1/012019}
}

% vertais
@article{ScrutinizingAutomatedLoadTesting,
abstract = {the current state of art and recent trends signify the applicability of software systems in innumerable contexts. The complete Software Engineering process encompasses various conventional testing techniques to avert malfunctions and ensure adequate quality. Many of the software systems when put into practice, provide services to hundreds and thousands of end users concurrently. Hence it is immensely important to conduct Load testing to affirm sustainability and scalability of such systems under huge load. The exposure of asynchronous load testing tools for building interactive testing processes gave precedence to the development of rich online load testing tools. There are various online load testing tools such as Neoload, Webload, OpenSTA, Apache JMeter, HP loadrunner and Test Studio. In this paper we have analyzed the performance of BlazeMeter, Apache JMeter, and HP loadrunner. The automated load testing tools use script for testing the applications, data services and apps. In this work, the performance evaluation of three load testing tools is achieved based on response time, number of samples, aggregate reports, latency, number of hits and throughput. The graphical representation of results perceptibly depicts the efficiency of BlazeMeter tool compared to Apache JMeter and HP loadrunner. As discussed in results section, BlazeMeter supports maximum number of samples with minimum response time, maximum number of hits and minimum latency. Hence our empirical study shows the proposed approach can decrease the cost of performance testing and helps to reveal potential performance issues.},
author = {Memon, Pirah and Dewani, Amirita and Bhatti, Sania and Hafiz, Tahseen},
address = {Gothenburg},
copyright = {Copyright International Journal of Advanced Studies in Computers, Science and Engineering 2018},
issn = {2278-7917},
journal = {International Journal of Advanced Studies in Computers, Science and Engineering},
keywords = {Automation ; Computer engineering ; Empirical analysis ; End users ; Graphical representations ; Malfunctions ; Performance evaluation ; Response time ; Software engineering ; Studies},
language = {eng},
number = {3},
pages = {35-42},
publisher = {International Journal of Advanced Studies in Computers, Science and Engineering},
title = {Scrutinzing Automated Load Testing via Blazmeter, Apache JMeter and HP LoadRunner},
volume = {7},
year = {2018},
}

% vertais
@article{UsabilityMetricsTrackingInterfaceImprovements,
abstract = {It is argued that most usability engineering is qualitative in nature: you observe how users use your product - what they have difficulty doing, what they like, and what they hate - then you refine your design accordingly. Given the severe time pressure in most development projects, it's enough to know that something is good or bad. Rarely is it necessary or possible to take the time to find out how good or bad, since if it's good you want to leave it alone and if it is bad you want to change it (if you have time, that is). There are some cases, though, where you do want numbers. The author does not recommend usability metrics for all projects; qualitative methods are usually sufficient to ensure substantial usability improvements. That would mean you won't know bow much better your new design is, but as long as it is not worse than the old one, going with the new one is clearly the better decision. However, organizations that do extensive software development and care about their engineering maturity should collect metrics for at least some of their projects.},
author = {Nielsen, Jakob},
address = {Los Alamitos},
issn = {0740-7459},
journal = {IEEE software},
keywords = {Software development ; Usability},
language = {eng},
number = {6},
pages = {1-2},
publisher = {IEEE},
title = {Usability metrics: tracking interface improvements},
volume = {13},
year = {1996},
doi={10.1109/MS.1996.8740869}
}

% vertais
@article{ExtractingUsabilityInformation,
abstract = {Modern window-based user interface systems generate user interface events as natural products of their normal operation. Because such events can be automatically captured and because they indicate user behavior with respect to an application's user interface, they have long been regarded as a potentially fruitful source of information regarding application usage and usability. However, because user interface events are typically voluminos and rich in detail, automated support is generally required to extract information at a level of abstraction that is useful to investigators interested in analyzing application usage or evaluating usability. This survey examines computer-aided techniques used by HCI practitioners and researchers to extract usability-related information from user interface events. A framework is presented to help HCI practitioners and researchers categorize and compare the approaches that have been, or might fruitfully be, applied to this problem. Because many of the techniques in the research literature have not been evaluated in practice, this survey provides a conceptual evaluation to help identify some of the relative merits and drawbacks of the various classes of approaches. Ideas for future research in this area are also presented. This survey addresses the following questions: How might user interface events be used in evaluating usability? How are user interface events related to other forms of usability data? What are the key challenges faced by investigators wishing to exploit this data? What approaches have been brought to bear on this problem and how do they compare to one another? What are some of the important open research questions in this area?},
author = {Hilbert, David and Redmiles, David},
address = {New York, NY},
copyright = {2001 INIST-CNRS},
issn = {0360-0300},
journal = {ACM computing surveys},
keywords = {Applied sciences ; Automation ; Computer aided software engineering ; Computer interfaces ; Computer science; control theory; systems ; Computer systems and distributed systems. User interface ; Evaluation ; Exact sciences and technology ; Human error ; Human-computer interaction ; Information sources ; Matrix ; Natural products ; sequential data analysis ; Software ; Software engineering ; Studies ; Usability ; usability testing ; User behavior ; User interface ; user interface event monitoring},
language = {eng},
number = {4},
pages = {384-421},
publisher = {ACM},
title = {Extracting usability information from user interface events},
volume = {32},
year = {2000},
doi = {10.1145/371578.371593}
}

% ei vertais
@book{AgileTestingAPracticalGuide,
author={Crispin, Lisa and Gregory, Janet},
address = {Upper Saddle River, NJ},
booktitle = {Agile testing: A practical guide for testers and agile teams},
isbn = {0-321-53446-8},
keywords = {Computer software -- Testing},
language = {eng},
lccn = {2008042444},
publisher = {Addison-Wesley},
series = {The Addison-Wesley signature series},
title = {Agile testing: A practical guide for testers and agile teams},
year = {2009}
}

% vertais
@article{EvaluatingAndImplementing,
author = {John Scarpino},
issn = {1529-7314},
journal = {Issues in information systems},
language = {eng},
number = {1},
pages = {1-10},
publisher = {IACIS},
title = {Evaluating and Implementing Load and Performance Testing Tools to Test Adobe Flex and Other Rich Internet Applications: A Case Study},
volume = {13},
year = {2012},
doi = {10.48009/1_iis_2012_1-10}
}

% vertais
@article{AFuzzyApproach,
abstract = {Performance of a software is an important feature to determine the quality of the software developed. Performance testing of modular software is a time consuming and costly task. Several performance testing tools (PTTs) are available in the market which help software developers to test their software performance. In this paper, we propose an integrated multiobjective optimization model for evaluation and selection of best-fit PTT for modular software system. The total performance tool cost is minimized and the fitness evaluation score of the PTTs is maximized. The fitness evaluation of PTT is done based on various attributes by making use of the Technique for Order Preference by Similarity to Ideal Solution (TOPSIS). The model allows the software developers to select the number of PTTs as per their requirement. The individual performance of the modules is considered based on some performance properties. The reusability constraints are considered, as a PTT can be used in the same module to test different properties and/or it can be used in different modules to test same or different performance properties. A real-world case study from the domain of enterprise resource planning (ERP) is used to show the working of the suggested optimization model.},
author = {Mehlawat, Mukesh Kumar and Mahajan, Divya},
address = {Singapore},
copyright = {2022, World Scientific Publishing Company},
issn = {0218-5393},
journal = {International journal of reliability, quality, and safety engineering},
keywords = {Computer software industry ; Enterprise resource planning ; Fitness ; Modular systems ; Modules ; Multiple objective analysis ; Optimization ; Software ; Software development ; Software development tools},
language = {eng},
number = {1},
publisher = {World Scientific Publishing Company},
title = {A fuzzy approach for evaluation and selection of performance testing tools for modular software development},
volume = {29},
year = {2022},
doi = {10.1142/S021853932150039X}
}

% ei vertais
@online{w3c,
  author = {Haas, Hugo and Brown, Allen},
  title = {Web Services Glossary},
  year = 2004,
  url = {https://www.w3.org/TR/ws-gloss/},
  urldate = {2023-03-05}
}

% ei kai vertais, kirjan luku
@Inbook{WhatIsSoftwarePerformance,
author="Cortellessa, Vittorio
and Di Marco, Antinisca
and Inverardi, Paola",
title="What Is Software Performance?",
bookTitle="Model-Based Software Performance Analysis",
year="2011",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="1--7",
abstract="The increasing complexity of software and its pervasiveness in everyday life has in the last years motivated growing interest for software analysis. This has mainly been directed to assess functional properties of the software systems (related to their structure and their behavior) and, in the case of safety critical systems, dependability properties. The quantitative behavior of a software system has gained relevance only recently with the advent of software performance analysis. This kind of analysis aims at assessing the quantitative behavior of a software system by comprehensively analyzing its structure and its behavior, from design to code. In this chapter we introduce the concepts and definitions that will be used throughout the entire book.",
isbn="978-3-642-13621-4",
doi="10.1007/978-3-642-13621-4_1"
}

% ei vertais
@online{AmazonRevenue,
  author = {Greg Linden},
  title = {Slides from my talk at Stanford},
  year = 2006,
  url = {http://glinden.blogspot.com/2006/12/slides-from-my-talk-at-stanford.html},
  urldate = {2023-03-05}
}

% vertais
@article{SoftwarePerformanceTesting,
abstract = {The article provides an overview of existing performance testing types. A test plan using the Jmeter tool is considered, the ability to compile tests and analyze them to obtain results is provided, while ensuring appropriate quality and ease of use. Software testing is one of the most important steps in software development. The universal approach to test development is complicated by the sheer data volume, the availability of various techniques and tools. Of particular importance is testing the computational performance of applications. Compared to other types of testing, this type is extremely complex and requires some personnel skills. Performance is mostly tested by automated methods, but statistical testing becomes important in parametric program research.},
author = {Olha M. Melkozerova and Sergiy G. Rassomakhin},
issn = {2304-6201},
journal = {Bulletin of V.N. Karazin Kharkiv National University, series «Mathematical modeling. Information technology. Automated control systems»},
language = {eng},
number = {45},
pages = {56-66},
publisher = {V. N. Karazin Kharkiv National University},
title = {Software performance testing},
year = {2020},
doi = {10.26565/2304-6201-2020-45-07}
}

% ei vertais
@book{ProApacheJMeter,
abstract = {Quickly ramp up your practical knowledge of Apache JMeter for software performance testing and focus on actual business problems. This step-by-step guide covers what you will need to know to write and execute test scripts, and verify the results.Pro Apache JMeter covers almost every aspect of Apache JMeter in detail and includes helpful screenshots and a case study. A performance primer chapter provides a high-level summary of terms used in performance testing on a day-to-day basis that also is useful for non-technical readers.A sample web application Digital Toys has been developed and test scripts are provided for you to try while progressing through the chapters.What You'll LearnCreate and execute an Apache JMeter test planInterpret the results of your test planUnderstand distributed testing using Apache JMeterUse Apache JMeter advanced features such as JDBC, REST, FTP, AJAX, SOAP, and mobile performance testingRead a sample case study covering end-to-end planning and execution of a performance testing projectGenerate and analyze a performance dashboardWho This Book Is ForSoftware performance testing professionals, quality assurance professionals, architects, engineers, project managers, product managers},
author = {Matam, Sai and Jain, Jagdeep},
address = {Berkeley, CA},
booktitle = {Pro Apache JMeter},
copyright = {Sai Matam and Jagdeep Jain 2017},
isbn = {9781484229606},
keywords = {Computer Science ; Open Source ; Professional and Applied Computing ; Web Development ; Web services},
language = {eng},
publisher = {Apress L. P},
title = {Pro Apache JMeter: Web Application Performance Testing},
year = {2017},
}

% vertais(?)
@inproceedings{ScrutinyOnVariousApproaches,
abstract = {Performance testing is vital for all types of systems and applications, especially in critical fields like Medical, Drug discovery systems and Healthcare, and also for Flight control, defense, Automotive etc. which are mission critical. The testing has to be started in the early stages of the product development cycle for better performance and quality. The performance testing tools tests the system or application rigorously before delivering it to customers and its efficiency builds confidence in users so that they always view the statistical results as accurate and genuine. This paper presents a comparison on the commonly used performance testing tools and concepts, and also the methodologies of testing for a variety of existing and emerging applications is explained.},
author = {Jacob, Anjali and Karthikevan, A.},
booktitle = {2018 Second International Conference on Electronics, Communication and Aerospace Technology (ICECA)},
isbn = {1538609657},
keywords = {Computer bugs ; Conferences ; Methodologies ; Performance Testing ; Product Development Cycle ; Security ; Software ; Software testing},
language = {eng},
pages = {509-515},
publisher = {IEEE},
title = {Scrutiny on Various Approaches of Software Performance Testing Tools},
year = {2018},
doi = {10.1109/ICECA.2018.8474876}
}

% vertais
@article{WebPerformanceTestingTools,
issn = {2321-9653},
journal = {International Journal for Research in Applied Science and Engineering Technology},
author={Shalini, Jawahar Thakur},
language = {eng},
number = {X},
pages = {1057-1063},
title = {Web Performance Testing Tools--A Review},
volume = {V},
year = {2017},
doi = {10.22214/ijraset.2017.10154}
}

% ei vertais
@online{WebLOAD,
  title = {WebLOAD Pricing},
  year = 2023,
  url = {https://www.radview.com/pricing/},
  urldate = {2023-03-25}
}

% ei vertais
@online{LoadNinja,
  title = {LoadNinja Pricing},
  year = 2023,
  url = {https://loadninja.com/pricing/},
  urldate = {2023-03-25}
}

% ei vertais
@online{Gatling,
  title = {Gatling Enterprise Pricing},
  year = 2023,
  url = {https://gatling.io/pricing/#self-hosted},
  urldate = {2023-03-25}
}

% vertais
@article{PerformanceTestingMethodologiesAndTools,
  title={Performance testing: methodologies and tools},
  author={Sarojadevi, H},
  journal={Journal of Information Engineering and Applications},
  volume={1},
  number={5},
  pages={5--13},
  year={2011}
}

% ei vertais
@online{Top27PerformanceTestingTools,
  author = {Amrita Pathak},
  title = {Top 27 Performance Testing Tools to Use in 2023},
  year = 2023,
  url = {https://kinsta.com/blog/performance-testing-tools/},
  urldate = {2023-03-25}
}

% ei vertais
@manual{NeoLoadThirdPartyTools,
  author = {Tricentis},
  title = {Integrate NeoLoad with third-party tools},
  titleaddon = {NeoLoad Documentation},
  version = {9.0},
  year = 2022,
  url = {https://documentation.tricentis.com/neoload/9.0/en/WebHelp/#11857.htm},
  urldate = {2023-03-25}
}

% ei vertais
@online{ITbudgets,
  author = {Justina Alexandra Sava},
  organization = {Statista},
  title = {IT budgets - Statistics \& Facts},
  year = 2022,
  url = {https://www.statista.com/topics/6198/it-budgets-and-investments},
  urldate = {2023-03-28}
}

% ei vertais
@online{Tools8ThingsToConsider,
  author = {Mike Urbanovich},
  title = {Performance testing tools},
  subtitle = {8 things to consider},
  year = {2021},
  url = {https://techbeacon.com/app-dev-testing/8-things-consider-about-performance-testing-tools},
  urldate = {2023-04-10}
}

% ei vertais
@online{HowToChooseTheRight,
  author = {PerfMatrix},
  title = {How to choose the right Performance Testing tool?},
  url = {https://www.perfmatrix.com/how-to-choose-the-right-performance-testing-tool/},
  year = {2020},
  urldate = {2023-04-10}
}

% ei vertais
@online{SOAPvsREST,
  author = {Alyssa Walker},
  title = {SOAP vs REST API},
  subtitle = {Difference Between Web Services},
  url = {https://www.guru99.com/comparison-between-web-services.html},
  year = {2023},
  urldate = {2023-04-10}
}

% ei vertais
@online{MQTTvsCoAP,
  author = {Ian Craggs},
  title = {SOAP vs REST API},
  url = {https://www.hivemq.com/blog/mqtt-vs-coap-for-iot/},
  year = {2022},
  urldate = {2023-04-10}
}
